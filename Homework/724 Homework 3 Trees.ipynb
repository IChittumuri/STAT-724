{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.Apply boosting, bagging, and random forests to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b3ff58aa9762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Tree plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydot'"
     ]
    }
   ],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tree plotting\n",
    "import pydot\n",
    "from IPython.display import Image\n",
    "import graphviz \n",
    "#from sklearn.externals.six import StringIO  \n",
    "from io import StringIO  \n",
    "\n",
    "# Model selection\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Trees\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates images of tree models using 'pydot' & 'export_graphviz'\n",
    "def print_tree(estimator, features, class_names=None, filled=True):\n",
    "    tree = estimator\n",
    "    names = features\n",
    "    color = filled\n",
    "    classn = class_names\n",
    "    \n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled)\n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "    return (graph)\n",
    "\n",
    "# Prints binary rules for a decision tree\n",
    "# from: # https://scikit-learn.org/dev/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py\n",
    "def tree_structure(clf):\n",
    "\n",
    "    n_nodes = clf.tree_.node_count\n",
    "    children_left = clf.tree_.children_left\n",
    "    children_right = clf.tree_.children_right\n",
    "    feature = clf.tree_.feature\n",
    "    threshold = clf.tree_.threshold\n",
    "\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        # `pop` ensures each node is only visited once\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "\n",
    "        # If the left and right child of a node is not the same we have a split node\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        # If a split node, append left and right children and depth to `stack` so we can loop through them\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    print(\"The binary tree structure has {n} nodes and has \"\n",
    "          \"the following tree structure:\\n\".format(n=n_nodes))\n",
    "    for i in range(n_nodes):\n",
    "        if is_leaves[i]:\n",
    "            print(\"{space}node={node} is a leaf node.\".format(\n",
    "                space=node_depth[i] * \"\\t\", node=i))\n",
    "        else:\n",
    "            print(\"{space}node={node} is a split node: \"\n",
    "                  \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "                  \"else to node {right}.\".format(\n",
    "                      space=node_depth[i] * \"\\t\",\n",
    "                      node=i,\n",
    "                      left=children_left[i],\n",
    "                      feature=feature[i],\n",
    "                      threshold=threshold[i],\n",
    "                      right=children_right[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "UP_DIR = '/Users/isabellachittumuri/Documents/Hunter College/Fall 2020/Stat 724/Jupyter/'\n",
    "csv_file = os.path.join(UP_DIR,'OJ.csv')\n",
    "\n",
    "OJ = pd.read_csv(csv_file, header=0, index_col=0, squeeze=True)\n",
    "OJ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make categorical outputs numerical\n",
    "OJ['Store7'] = OJ.Store7.map({'Yes':1,'No':0})\n",
    "OJ['Purchase'] = OJ.Purchase.map({'CH':1,'MM':0})\n",
    "OJ['Purchase'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OJ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = OJ[['LoyalCH','PriceDiff']].values\n",
    "y = np.log(OJ.Purchase.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1070, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 80% of data in X_train, 20% of data in X_test\n",
    "print('train size: {0}  test size {1}'.format(X_train.shape[0],X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(11,4))\n",
    "ax1.hist(OJ.Purchase.values)\n",
    "ax1.set_xlabel('Salary')\n",
    "ax2.hist(y)\n",
    "ax2.set_xlabel('Log(Salary)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree with 3 leaf nodes. Note that we fix the random state (seed) for reproducibility.\n",
    "regr = DecisionTreeRegressor(max_leaf_nodes=3, random_state=0)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple plot\n",
    "import sklearn.tree as tree\n",
    "tree.plot_tree(regr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw graph\n",
    "graph, = print_tree(regr, features = ['LoyalCH','PriceDiff']) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the tree structure\n",
    "\n",
    "tree_structure(regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data partition into 3 regions\n",
    "\n",
    "OJ.plot('LoyalCH','PriceDiff', kind='scatter', color='orange', figsize=(7,6))\n",
    "plt.xlim(0,25)\n",
    "plt.ylim(ymin=-5)\n",
    "plt.xticks([1, 4.5, 24])\n",
    "plt.yticks([1, 117.5, 238])\n",
    "plt.vlines(4.5, ymin=-5, ymax=250)\n",
    "plt.hlines(117.5, xmin=4.5, xmax=25)\n",
    "plt.annotate('R1', xy=(2,117.5), fontsize='xx-large')\n",
    "plt.annotate('R2', xy=(11,60), fontsize='xx-large')\n",
    "plt.annotate('R3', xy=(11,170), fontsize='xx-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to train and test and create a sequence of trees corresponding to sequence of values of 'alpha', the cost-complexity parameter (ccp):\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "regr2 = DecisionTreeRegressor(random_state=0)\n",
    "path = regr2.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) \"Impurity\"**:  In the following plot, the maximum effective alpha value is removed, because it is the trivial tree with only one node. In the case of a regression tree, \"impurity\" is the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    reg = DecisionTreeRegressor(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    reg.fit(X_train, y_train)\n",
    "    regs.append(reg)\n",
    "\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(regs[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Depth and number of nodes**: \n",
    "Here we show that the number of nodes and tree depth decreases as alpha increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = regs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "node_counts = [reg.tree_.node_count for reg in regs]\n",
    "depth = [reg.tree_.max_depth for reg in regs]\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "# ax = plt.subplots(2, 1)\n",
    "# figure = plt.figure(figsize=(20, 10))\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\n",
    "ax[0].set_xlabel(\"alpha\")\n",
    "ax[0].set_ylabel(\"number of nodes\")\n",
    "ax[0].set_title(\"Number of nodes vs alpha\")\n",
    "\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\n",
    "ax[1].set_xlabel(\"alpha\")\n",
    "ax[1].set_ylabel(\"depth of tree\")\n",
    "ax[1].set_title(\"Depth vs alpha\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Now, demonstrate **overfitting** by plotting the accuracy vs. alpha of trees built on train and test sets, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [reg.score(X_train, y_train) for reg in regs]\n",
    "test_scores = [reg.score(X_test, y_test) for reg in regs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sc = np.array(test_scores)\n",
    "ind_best = test_sc.argmax()\n",
    "print(max(train_scores))\n",
    "ccp_alphas[ind_best], test_sc[ind_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CV to select best CCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the first step of prunning (see just before (a) above)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "regr2 = DecisionTreeRegressor(random_state=0)\n",
    "path = regr2.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for fitting trees for diiferent values of the cost-complexity parameter (CCP) on the training data using cross-validation\n",
    "def run_cv_on_trees(X, y, ccp_alphas, cv, scoring, treetype='classifier'):\n",
    "    cv_scores_list = []\n",
    "    cv_scores_std = []\n",
    "    cv_scores_mean = []\n",
    "    scores = []\n",
    "    \n",
    "    i = 0\n",
    "    for alpha in ccp_alphas:\n",
    "        if treetype == 'classifier':\n",
    "            tree_model = DecisionTreeClassifier(ccp_alpha=alpha)\n",
    "            #scoring = 'accuracy' if scoring is None else scoring \n",
    "        else:\n",
    "            tree_model = DecisionTreeRegressor(ccp_alpha=alpha)\n",
    "            #scoring = 'mse' if scoring is None else scoring\n",
    "\n",
    "        cv_scores = cross_val_score(tree_model, X, y, cv=cv, scoring=scoring)\n",
    "        #if treetype=='regression':\n",
    "        #    if i==0:\n",
    "        #        cv_root = cv_scores \n",
    "        #    cv_scores = cv_scores/cv_root\n",
    "        cv_scores_list.append(cv_scores)\n",
    "        cv_scores_mean.append(cv_scores.mean())\n",
    "        cv_scores_std.append(cv_scores.std())\n",
    "        scores.append(tree_model.fit(X, y).score(X, y))\n",
    "        i+=1\n",
    "        \n",
    "    cv_scores_mean = np.array(cv_scores_mean)\n",
    "    cv_scores_std = np.array(cv_scores_std)\n",
    "    scores = np.array(scores)\n",
    "    return cv_scores_mean, cv_scores_std, scores\n",
    "\n",
    "# function for plotting cross-validation results\n",
    "def plot_cv_on_trees(ccp_alphas, cv_scores_mean, cv_scores_std, scores, title):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
    "    ax.plot(ccp_alphas, cv_scores_mean, '-o', label='mean cross-validation score', alpha=0.9)\n",
    "    ax.fill_between(ccp_alphas, cv_scores_mean-1*cv_scores_std, cv_scores_mean+1*cv_scores_std, alpha=0.2)\n",
    "    ylim = plt.ylim()\n",
    "    ax.plot(ccp_alphas, scores, '-*', label='train score', alpha=0.9)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('CCP (alpha)', fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xticks(ccp_alphas)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting multiple trees for each alpha\n",
    "sm_ccp = ccp_alphas #range(1,25)\n",
    "sm_cv_scores_mean, sm_cv_scores_std, sm_scores = run_cv_on_trees(X_train, y_train, sm_ccp, cv=10, scoring=None, treetype='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting score (MSE or Accuracy)\n",
    "plot_cv_on_trees(sm_ccp, sm_cv_scores_mean, sm_cv_scores_std, sm_scores, 'Score per ccp (alpha) on training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in \n",
    "sm_ccp = ccp_alphas[-15:] #range(1,25)\n",
    "sm_cv_scores_mean, sm_cv_scores_std, sm_scores = run_cv_on_trees(X_train, y_train, sm_ccp, cv=10, scoring=None, treetype='regression')\n",
    "plot_cv_on_trees(sm_ccp, sm_cv_scores_mean, sm_cv_scores_std, sm_scores, 'Score per ccp (alpha) on training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_best = sm_cv_scores_mean.argmin()\n",
    "best_alpha = sm_ccp[idx_best]\n",
    "sm_best_tree_cv_score = sm_cv_scores_mean[idx_best]\n",
    "sm_best_tree_cv_score_std = sm_cv_scores_std[idx_best]\n",
    "print('The tree with alpha={} at step={} achieves the best mean cross-validation score {} +/- {} on training dataset'.format(\n",
    "      round(best_alpha,7), idx_best, round(sm_best_tree_cv_score, 5), round(sm_best_tree_cv_score_std, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regr = DecisionTreeRegressor(ccp_alpha=ccp_alphas[1]).fit(X_train, y_train)\n",
    "graph, = print_tree(best_regr, features = ['LoyalCH','PriceDiff']) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The best pruned tree is a stump\n",
    "best_regr = DecisionTreeRegressor(ccp_alpha=best_alpha).fit(X_train, y_train)\n",
    "graph, = print_tree(best_regr, features = ['LoyalCH','PriceDiff'])\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = clf.predict(X_test)\n",
    "\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_test, pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    print('True Negatives: ', cm[0][0])\n",
    "    print('False Positives: ', cm[0][1])\n",
    "    print('False Negatives: ', cm[1][0])\n",
    "    print('True Positives: ', cm[1][1])\n",
    "    print('Total Positive (Actual): ', np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to predict slk model using X_test perdictors\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, pred).T, index=['No', 'Yes'], columns=['No', 'Yes'])\n",
    "cm.index.name = 'Predicted'\n",
    "cm.columns.name = 'True'\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision of the model using test data is 72% in the large class\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One measure of success\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
